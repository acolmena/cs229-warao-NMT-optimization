# -*- coding: utf-8 -*-
"""m2m_100_418M_embedding_extended

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vWN4rMDF58Vl1UwdhXpGjkH47L2Zctwu

# M2M-100 Full Finetuning with Extended Embedding Matrix
This a script that performs full finetuning of M2M-100-418M on our data.

It leverages the ML HuggingFace classes for training a sequence model and tokenizing.

This code is adapted from the https://github.com/masakhane-io/lafand-mt/tree/main.

And from https://www.cs.columbia.edu/~johnhew//vocab-expansion.html

**Citations**
> Adelani, D., Alabi, J., Fan, A., Kreutzer, J., Shen, X., Reid, M., Ruiter, D., Klakow, D., Nabende, P., Chang, E., Gwadabe, T., Sackey, F., Dossou, B. F. P., Emezue, C., Leong, C., Beukman, M., Muhammad, S., Jarso, G., Yousuf, O., Niyongabo Rubungo, A., … Manthalu, S. (2022). A few thousand translations go a long way! Leveraging pre-trained models for African news translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 3053–3070). Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.naacl-main.223

> Hewitt, J. (2021, December 6). Initializing new word embeddings for pretrained language models [Web page]. Columbia University. https://www.cs.columbia.edu/~johnhew//vocab-expansion.html
"""

# !pip install sacrebleu==2.0.0
# !pip install protobuf
# !pip show sentencepiece
# !pip install peft accelerate
# !pip install evaluate

import os
import sys
import logging
import numpy as np
from datasets import load_dataset
from evaluate import load
from transformers import (
    AutoConfig,
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    set_seed,
)
import torch
import random
import pandas as pd
import json  

random.seed(42) # reproducibility



model_configs = {
    "facebook/m2m100_418M": ("pt", "es"),   # order: src, target
    "facebook/m2m100_1.2B": ("pt", "es"),
    "facebook/mbart-large-50": ("pt_XX", "es_XX"),
    "facebook/mbart-large-50-many-to-many-mmt": ("pt_XX", "es_XX"),
    "facebook/nllb-200-distilled-600M": ("wro_Latn", "spa_Latn"),  # fake source code for this model seems to work best
    "google/mt5-base": ("Warao", "Spanish"),
    "google/mt5-small": ("Warao", "Spanish"),
    "google/byt5-base": ("Warao", "Spanish"),
    "google/byt5-small": ("Warao", "Spanish"),

}

models = list(model_configs.keys())
MODEL_NAME = models[4]  # change index to try different models
MODEL = MODEL_NAME.split('/')[1]  # simple name version of model for output file of predictions
TRAIN_FILE = "./input/parallel_train.csv"
VAL_FILE = "./input/parallel_val.csv"
TEST_FILE = "./input/parallel_test.csv"
OUTPUT_DIR = f"./{MODEL}-extd-em-full-ft"
SOURCE_CODE = model_configs[MODEL_NAME][0]
TARGET_CODE = model_configs[MODEL_NAME][1]
MAX_LEN = 128
EPOCHS = 3
BATCH_SIZE = 8
LEARNING_RATE = 1e-4
WEIGHT_DECAY = 0.01
WARMUP_STEPS = 0
LOG_STEPS = 100
NUM_BEAMS = 4

do_predictions = True
do_pred_on_test = True
do_evaluate = True
do_eval_on_test = True

TOKEN_FILE = './input/warao_tokenizer_parallel.json'
VERSION = 'v2'  # 'v1' for Warao vocab list, 'v2' for BPE tokenizer json file
NUM_NEW_WARAO_TOKENS = 6000  # number of new Warao tokens we'll add to tokenizer
TOK_START_IDX = 95  # index non special tokens start in vocab

WANDB_RUN_NAME = f"{MODEL}_extd_em_full_ft"
WANDB_PROJECT = "cs229-experiments"
EXTEND_EMBEDDING = True   # whether to extend embedding matrix with new tokens



device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
print("USING DEVICE:", device)

def load_new_tokens(token_file, version='v1'):
    # v1) load Warao vocabulary
    if version == 'v1':
        warao_vocab = pd.read_csv(token_file)
        warao_vocab_lst = list(warao_vocab['warao_words'])

        # randomly sample 200 tokens/words in the vocab
        num_wrds = 200
        new_toks = random.sample(warao_vocab_lst, k=num_wrds)

    # v2) load BPE tokens json file
    elif version == 'v2':
        with open(token_file, 'r', encoding='utf-8') as f:
            token_data = json.load(f)
        new_toks = random.sample(list(token_data['model']['vocab'].keys())[TOK_START_IDX:], NUM_NEW_WARAO_TOKENS)  # skip special tokens
     

    print(f"Preview vocab: {new_toks[:5]}")
    return new_toks


# FBTs depend on the model type
def get_forced_bos_token_id(model_name, tokenizer, target_lang):
    if "m2m100" in model_name:
        return tokenizer.get_lang_id(target_lang)
    elif "mbart" in model_name:
        return tokenizer.lang_code_to_id[target_lang]
    elif "nllb" in model_name:
        return tokenizer.convert_tokens_to_ids(target_lang)
    else:
        # T5 models don't have forced_bos_token_id -- make this default behavior
        return None



def start_training(model_name_or_path, train_file, val_file, test_file, output_dir,
                  source_lang=None, target_lang=None,
                  max_source_length=400, max_target_length=400,
                  num_train_epochs=3, batch_size=8, learning_rate=1e-5, num_beams=4,
                  do_predictions=True, do_pred_on_test=False,
                  do_evaluate=True, do_eval_on_test=False, wandb_project="full_ft",
                  wandb_run_name=None, weight_decay=0.01, warmup_steps=0, use_wandb=False,
                  logging_steps=100, token_file=None, version='v1'):

    # log
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)

    print("\n" + "=" * 50)
    logger.info("Loading datasets . . . ")
    print("=" * 50)
    data_files = {"train": train_file, "validation": val_file, "test": test_file}
    raw_datasets = load_dataset("csv", data_files=data_files)

    # preview datasets
    print("\n" + "=" * 50)
    print("Previewing datasets . . . ")
    print("=" * 50)
    for split, dataset in raw_datasets.items():
        print(f"{split}: {dataset[:0]}")


    # 1) load tokenizer
    print("\n" + "=" * 50)
    print(f"Loading {model_name_or_path} model and tokenizer . . . ")
    print("=" * 50)
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
    og_vocab_size = len(tokenizer)
    print(f"Original vocab size: {og_vocab_size}")

    

    # 3) extend embedding matrix -------------------------------------------------------
    if EXTEND_EMBEDDING:
        print("\n" + "=" * 50)
        print("Extending embedding matrix . . . ")
        print("=" * 50)
        # a. Load pretrained model
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)
        print(f"Original embedding matrix size: {model.get_input_embeddings().weight.size()}")

        # b. load new tokens 
        new_toks = load_new_tokens(token_file=token_file, version=version)

        # c. Add new tokens/words
        num_added = tokenizer.add_tokens(new_toks)
        print("Added this number of words/tokens to vocab:", num_added)

        # d. Resize embeddings
        model.resize_token_embeddings(len(tokenizer))

        # e. follow the instructions from:
        # compute the distribution from which we’ll sample our new embeddings
        tokens_added = len(new_toks)
        print(f"Extending embedding matrix by {tokens_added} tokens . . .")
        params = model.state_dict()
        embeddings = params['model.shared.weight']
        pre_expansion_embeddings = embeddings[:-3,:]
        mu = torch.mean(pre_expansion_embeddings, dim=0)
        n = pre_expansion_embeddings.size()[0]
        sigma = ((pre_expansion_embeddings - mu).T @ (pre_expansion_embeddings - mu)) / n
        dist = torch.distributions.multivariate_normal.MultivariateNormal(
                mu, covariance_matrix=1e-5*sigma)

        # f. load in new embeddings into model
        new_embeddings = torch.stack(tuple((dist.sample() for _ in range(3))), dim=0)
        embeddings[-3:,:] = new_embeddings
        params['model.shared.weight'][-3:,:] = new_embeddings
        model.load_state_dict(params)

        print(f"New embedding matrix size: {model.get_input_embeddings().weight.size()}")
        print(f"New vocab (last 10 tokens): {list(tokenizer.get_vocab().keys())[-10:]}" )
        print(f"New vocab size: {len(tokenizer)}")

    # breakpoint()

    # -----------------------------------------------------------------------------------

    # set language codes based on model type
    # check if model is a T5-based model
    is_t5_model = "mt5" in model_name_or_path or "byt5" in model_name_or_path

    # T5-based models dont have language codes
    if not is_t5_model:
        tokenizer.src_lang = source_lang
        tokenizer.tgt_lang = target_lang

        forced_bos_token_id = get_forced_bos_token_id(model_name_or_path, tokenizer, target_lang)
        if forced_bos_token_id is not None:
            model.config.forced_bos_token_id = forced_bos_token_id
    else:
        if model.config.decoder_start_token_id is None:
            model.config.decoder_start_token_id = tokenizer.pad_token_id

    # preprocess raw data -- tokenize + add prefix if necessary
    def preprocess_function(examples):
        inputs = examples["warao_sentence"]
        targets = examples["spanish_sentence"]

        # T5 models need task prefix
        if is_t5_model:
            inputs = [f"translate {source_lang} to {target_lang}: {text}" for text in inputs]

        model_inputs = tokenizer(inputs, max_length=max_source_length, truncation=True, padding=False)
        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=False)
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

    bleu_metric = load("sacrebleu")
    chrf_metric = load("chrf")

    def postprocess_text(preds, labels):
        preds = [p.strip() for p in preds]
        labels = [[l.strip()] for l in labels]
        return preds, labels

    def compute_metrics(eval_preds):  # adapted from lafand.md
        preds, labels = eval_preds
        if isinstance(preds, tuple):
            preds = preds[0]
        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

        # Replace -100 in the labels as we can't decode them
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

        # Post-processing
        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

        bleu_res = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)
        result = {"bleu": bleu_res["score"]}

        result["chrf"] = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)["score"]
        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
        result["gen_len"] = np.mean(prediction_lens)
        result = {k: round(v, 4) for k, v in result.items()}
        return result
    
    # if is_t5_model and batch_size > 8:
    #     gradient_accumulation_steps = batch_size // 8
    #     effective_batch_size = 8
    #     logger.info(f"Using gradient accumulation: {gradient_accumulation_steps} steps for effective batch size {batch_size}")


    training_args = Seq2SeqTrainingArguments(
        output_dir=output_dir,
        eval_strategy="epoch",
        save_strategy="epoch",
        learning_rate=learning_rate,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        weight_decay=weight_decay,
        save_total_limit=1,
        num_train_epochs=num_train_epochs,
        predict_with_generate=True,
        generation_max_length=max_target_length,
        generation_num_beams=num_beams,
        logging_dir=os.path.join(output_dir, "logs"),
        logging_steps=logging_steps,
        report_to="wandb" if use_wandb else "none",
        run_name=wandb_run_name,
        load_best_model_at_end=True,
        metric_for_best_model="bleu",
        greater_is_better=True,
        gradient_checkpointing=True if "nllb" in model_name_or_path or "mbart" in model_name_or_path else False, # trade comp time for memory
        fp16=True if "nllb" in model_name_or_path or "mbart" in model_name_or_path else False, # use mixed precision for large models
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    logger.info(f"Training . . .")
    trainer.train()

    trainer.save_model()
    logger.info("Fully Finetuned model saved!")

    # run evaluation
    if do_evaluate:
        logger.info(f"*** Doing Evaluations on {'test' if do_eval_on_test else 'validation'} set ***")
        if do_eval_on_test:
            eval_dataset = tokenized_datasets["test"]
        else:
            eval_dataset = tokenized_datasets["validation"]
        eval_results = trainer.evaluate(eval_dataset)
        logger.info(f"BLEU on validation set: {eval_results}")

    # generate predictions
    def do_predictions_func(tok_dataset, reg_dataset, trainer, auto_tokenizer):
        logger.info("*** Generating Predictions ***")

        predict_results = trainer.predict(
            tok_dataset,
            metric_key_prefix="predict",
            num_beams=num_beams,
        )
        preds = auto_tokenizer.batch_decode(predict_results.predictions, skip_special_tokens=True)
        preds = [p.strip().replace("\n", " ") for p in preds]

        logger.info(f"Metrics: {predict_results.metrics}")

        pred_output_file = f'{MODEL}_ft_predictions.csv'
        df = pd.DataFrame(reg_dataset)
        df['predictions'] = preds

        os.makedirs(output_dir, exist_ok=True)
        pred_file = os.path.join(output_dir, pred_output_file)
        df.to_csv(pred_file, index=False)
        logger.info(f"*** Predictions on {'test' if do_eval_on_test else 'validation'} set saved to {pred_file} ***")
        return preds, predict_results[1:]

    if do_predictions:
        if do_pred_on_test:
            tok_pred_dataset = tokenized_datasets["test"]
            reg_pred_dataset = raw_datasets["test"]
        else:
            tok_pred_dataset = tokenized_datasets["validation"]
            reg_pred_dataset = raw_datasets["validation"]

        do_predictions_func(
            tok_dataset=tok_pred_dataset,
            reg_dataset=reg_pred_dataset,
            trainer=trainer,
            auto_tokenizer=tokenizer,
        )



    # free up memory
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return


if __name__ == "__main__":
    # set seed for reproducibility
    set_seed(42)

    # start training
    start_training(
        model_name_or_path=MODEL_NAME,
        train_file=TRAIN_FILE,
        val_file=VAL_FILE,
        test_file=TEST_FILE,
        output_dir=OUTPUT_DIR,
        source_lang=SOURCE_CODE,
        target_lang=TARGET_CODE,
        max_source_length=MAX_LEN,
        max_target_length=MAX_LEN,
        num_train_epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        learning_rate=LEARNING_RATE,
        num_beams=NUM_BEAMS,
        weight_decay=WEIGHT_DECAY,
        warmup_steps=WARMUP_STEPS,
        do_predictions=do_predictions,
        do_pred_on_test=do_pred_on_test,
        do_evaluate=do_evaluate,
        do_eval_on_test=do_eval_on_test,
        wandb_project=WANDB_PROJECT,
        wandb_run_name=WANDB_RUN_NAME,
        use_wandb=True,
        logging_steps=LOG_STEPS,
        token_file=TOKEN_FILE,
        version=VERSION,
    )