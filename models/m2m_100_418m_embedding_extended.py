# -*- coding: utf-8 -*-
"""m2m_100_418M_embedding_extended

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vWN4rMDF58Vl1UwdhXpGjkH47L2Zctwu

# M2M-100 Full Finetuning with Extended Embedding Matrix
This a script that performs full finetuning of M2M-100-418M on our data.

It leverages the ML HuggingFace classes for training a sequence model and tokenizing.

This code is adapted from the https://github.com/masakhane-io/lafand-mt/tree/main.

And from https://www.cs.columbia.edu/~johnhew//vocab-expansion.html

**Citations**
> Adelani, D., Alabi, J., Fan, A., Kreutzer, J., Shen, X., Reid, M., Ruiter, D., Klakow, D., Nabende, P., Chang, E., Gwadabe, T., Sackey, F., Dossou, B. F. P., Emezue, C., Leong, C., Beukman, M., Muhammad, S., Jarso, G., Yousuf, O., Niyongabo Rubungo, A., … Manthalu, S. (2022). A few thousand translations go a long way! Leveraging pre-trained models for African news translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 3053–3070). Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.naacl-main.223

> Hewitt, J. (2021, December 6). Initializing new word embeddings for pretrained language models [Web page]. Columbia University. https://www.cs.columbia.edu/~johnhew//vocab-expansion.html
"""

MODEL_NAME = "facebook/m2m100_418M"
TRAIN_FILE = "parallel_train.csv"
VAL_FILE = "parallel_val.csv"
TEST_FILE = "parallel_test.csv"
OUTPUT_DIR = "./m2m100-418M-finetuned-warao-es"

!pip install sacrebleu==2.0.0
!pip install protobuf
!pip show sentencepiece
!pip install peft accelerate
!pip install evaluate

import os
import sys
import logging
import numpy as np
from datasets import load_dataset
from evaluate import load
from transformers import (
    AutoConfig,
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    set_seed,
)
import torch
import random
import pandas as pd

random.seed(42) # for reproducibility

device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
print("USING DEVICE:", device)

def start_training(model_name_or_path, train_file, val_file, test_file, output_dir,
                  source_lang="pt_XX", target_lang="es_XX",
                  max_source_length=128, max_target_length=128,
                  num_train_epochs=3, batch_size=8, learning_rate=1e-5, num_beams=4):

  # log
  logging.basicConfig(
      format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
      datefmt="%m/%d/%Y %H:%M:%S",
      handlers=[logging.StreamHandler(sys.stdout)],
  )
  logger = logging.getLogger(__name__)
  logger.setLevel(logging.INFO)

  print("\n" + "=" * 50)
  print("Loading datasets . . . ")
  print("=" * 50)
  data_files = {"train": train_file, "validation": val_file, "test": test_file}
  raw_datasets = load_dataset("csv", data_files=data_files)

  # preview datasets
  print("\n" + "=" * 50)
  print("Previewing datasets . . . ")
  print("=" * 50)
  for split, dataset in raw_datasets.items():
      print(f"{split}: {dataset[:5]}")


  # 1) load tokenizer
  print("\n" + "=" * 50)
  print(f"Loading {model_name_or_path} model and tokenizer . . . ")
  print("=" * 50)
  tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)

  # 2) load tokens
  warao_vocab = pd.read_csv('warao_vocab.csv')
  warao_vocab_lst = list(warao_vocab['warao_words'])
  print(f"Preview vocab: {warao_vocab_lst[:5]}")

  # randomly sample 200 tokens/words in the vocab
  num_wrds = 200
  rando_warao_words = random.sample(warao_vocab_lst, k=num_wrds)

  # 2. Load pretrained model
  model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)

  # 3. Add new tokens/words
  num_added = tokenizer.add_tokens(rando_warao_words)
  print("Added tokens:", num_added)

  # 4. Resize embeddings
  model.resize_token_embeddings(len(tokenizer))

  # 5) follow the instructions from:
  # compute the distribution from which we’ll sample our new embeddings
  tokens_added = len(rando_warao_words)
  params = model.state_dict()
  embeddings = params['model.shared.weight']
  pre_expansion_embeddings = embeddings[:-3,:]
  mu = torch.mean(pre_expansion_embeddings, dim=0)
  n = pre_expansion_embeddings.size()[0]
  sigma = ((pre_expansion_embeddings - mu).T @ (pre_expansion_embeddings - mu)) / n
  dist = torch.distributions.multivariate_normal.MultivariateNormal(
          mu, covariance_matrix=1e-5*sigma)

  #  load in new embeddings into model
  new_embeddings = torch.stack(tuple((dist.sample() for _ in range(3))), dim=0)
  embeddings[-3:,:] = new_embeddings
  params['model.shared.weight'][-3:,:] = new_embeddings
  model.load_state_dict(params)

  config = AutoConfig.from_pretrained(model_name_or_path)

  # set language codes
  tokenizer.src_lang = source_lang
  tokenizer.tgt_lang = target_lang
  model.config.forced_bos_token_id = tokenizer.lang_code_to_id[target_lang]

  # tokenizing
  def preprocess_function(examples):
      inputs = examples["warao_sentence"]
      targets = examples["spanish_sentence"]
      model_inputs = tokenizer(inputs, max_length=max_source_length, truncation=True)
      labels = tokenizer(targets, max_length=max_target_length, truncation=True)
      model_inputs["labels"] = labels["input_ids"]
      return model_inputs

  tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

  # data collator to handle different sizes of sentences
  data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

  metric = load("sacrebleu")

  def postprocess_text(preds, labels):
      preds = [p.strip() for p in preds]
      labels = [[l.strip()] for l in labels]
      return preds, labels

  def compute_metrics(eval_preds):
      preds, labels = eval_preds
      if isinstance(preds, tuple):
          preds = preds[0]
      decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
      labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
      decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
      decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)
      result = metric.compute(predictions=decoded_preds, references=decoded_labels)
      return {"bleu": round(result["score"], 4)}


  training_args = Seq2SeqTrainingArguments(
      output_dir=output_dir,
      eval_strategy="epoch",
      save_strategy="epoch",
      learning_rate=learning_rate,
      per_device_train_batch_size=batch_size,
      per_device_eval_batch_size=batch_size,
      weight_decay=0.01,
      save_total_limit=1,
      num_train_epochs=num_train_epochs,
      predict_with_generate=True,
      generation_max_length=max_target_length,
      generation_num_beams=num_beams,
      logging_dir=os.path.join(output_dir, "logs"),
      logging_steps=100,
  )

  trainer = Seq2SeqTrainer(
      model=model,
      args=training_args,
      train_dataset=tokenized_datasets["train"],
      eval_dataset=tokenized_datasets["validation"],
      tokenizer=tokenizer,
      data_collator=data_collator,
      compute_metrics=compute_metrics,
  )

  print("\n" + "=" * 50)
  print("Finetuning . . . ")
  print("=" * 50)
  trainer.train()
  trainer.save_model()
  print("Model saved!")

  # evaluate on test set
  # evaluate  # comment out for now because we evaluate at the end of every epoch so this gives the same result as epoch 3 print statement
  eval_results = trainer.evaluate(tokenized_datasets["validation"])
  logger.info(f"BLEU on validation set: {eval_results}")

  return tokenized_datasets["validation"], trainer, tokenizer

tokenized_dataset_val, trainer, auto_tokenizer = start_training(
    model_name_or_path=MODEL_NAME,
    train_file=TRAIN_FILE,
    val_file=VAL_FILE,
    test_file=TEST_FILE,
    output_dir=OUTPUT_DIR,
    source_lang="pt",    # fake code for Warao, this was suggested by the Few Thousand Translations paper
    target_lang="es",    # Spanish code
    num_train_epochs=3,
    batch_size=8,
    learning_rate=1e-4,
    num_beams=4,
)

!pwd
!ls -R /content

"""## Generating Predictions"""

# commenting this out for now since it's not working well

def generate_predictions(output_dir, tokenized_dataset_valid, trainer, auto_tokenizer):
   # log
  # logging.basicConfig(
  #     format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
  #     datefmt="%m/%d/%Y %H:%M:%S",
  #     handlers=[logging.StreamHandler(sys.stdout)],
  # )
  # logger = logging.getLogger(__name__)
  # logger.setLevel(logging.INFO)


  # create predictions for 500 random examples in test set for later evaluative use
  # random.seed(21)
  # num_samples = 500
  available_sample_size = len(tokenized_dataset_valid['warao_sentence'])

  # sample indices
  # sample_indices = random.sample(range(available_sample_size), min(num_samples, available_sample_size))

  # *** CHANGE BELOW FUNCTION TO FILTER OUT ROWS WITH INDEX IN "sample_indices"***
  sampled_test_pairs = random.sample(tokenized_dataset_valid['warao_sentence'], min(available_sample_size, len(tokenized_dataset_valid['spanish_sentence'])))
  predict_results = trainer.predict(sampled_test_pairs)
  preds = auto_tokenizer.batch_decode(predict_results.predictions, skip_special_tokens=True)
  preds = [p.strip().replace("\n", " ") for p in preds]

  # save predictions
  os.makedirs(output_dir, exist_ok=True)
  pred_file = os.path.join(output_dir, "mBART50_predictions.txt")
  with open(pred_file, "w", encoding="utf-8") as f:
      f.write("\n".join(preds))

  # logger.info(f"Predictions saved to {pred_file}")

# commenting this out for now since it's not working well

print('\n' + '=' * 50)
print('Generating predictions . . .')
print('=' * 50)

generate_predictions(
    output_dir="./mbart50-finetuned-warao-es",
    tokenized_dataset_val=tokenized_dataset_val,
    trainer=trainer,
    auto_tokenizer=auto_tokenizer,
    )